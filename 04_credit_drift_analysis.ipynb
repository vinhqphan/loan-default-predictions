{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_historical_clean = pd.read_csv('/Users/vinh/FS/thesis/data/df_historical_clean.csv')\n",
    "df_historical_clean.loc[df_historical_clean['term'] == 36, 'term'] = 0\n",
    "df_historical_clean.loc[df_historical_clean['term'] == 60, 'term'] = 1\n",
    "df_historical_clean.loc[df_historical_clean['loan_status'] == 'Fully Paid', 'loan_status'] = 0\n",
    "df_historical_clean.loc[df_historical_clean['loan_status'] == 'Charged Off', 'loan_status'] = 1\n",
    "year = pd.to_datetime(df_historical_clean['issue_d']).dt.year\n",
    "df_historical_clean.insert(2, 'year', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_by_year = pd.DataFrame(df_historical_clean['year'].value_counts(normalize = True))\n",
    "loans_by_year = loans_by_year.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_df_prep(df, year1, year2):\n",
    "    df_year1 = df.loc[df['year'] == year1]\n",
    "    df_both_years = df.loc[df['year'].isin([year1, year2])]\n",
    "    \n",
    "    X_year1 = df_year1.drop('loan_status', axis = 1)\n",
    "    y_year1 = df_year1[['loan_status']]\n",
    "    x_train_year1, x_test_year1, y_train_year1, y_test_year1 = train_test_split(X_year1, y_year1, test_size = 0.2, random_state = 1337, stratify = y_year1)\n",
    "    temp = pd.concat([y_train_year1, x_train_year1], axis = 1)\n",
    "    defaults_year1 = temp.loc[temp['loan_status'] == 1]\n",
    "    completed_year1 = temp.loc[temp['loan_status'] == 0]\n",
    "    defaults_train_year1, defaults_test_year1 = train_test_split(defaults_year1, test_size = 0.2, random_state = 1337)\n",
    "    completed_train_year1, completed_test_year1 = train_test_split(completed_year1,\n",
    "                                                                   test_size = defaults_train_year1.shape[0] / completed_year1.shape[0],\n",
    "                                                                   random_state = 1337)\n",
    "    year1_balanced = pd.concat([completed_test_year1, defaults_train_year1])\n",
    "\n",
    "    X_both_years = df_both_years.drop('loan_status', axis = 1)\n",
    "    y_both_years = df_both_years[['loan_status']]\n",
    "    x_train_both_years, x_test_both_years, y_train_both_years, y_test_both_years = train_test_split(X_both_years, y_both_years, test_size = 0.2, random_state = 1337, stratify = y_both_years)\n",
    "    temp2 = pd.concat([y_train_both_years, x_train_both_years], axis = 1)\n",
    "    defaults_both_years = temp2.loc[temp2['loan_status'] == 1]\n",
    "    completed_both_years = temp2.loc[temp2['loan_status'] == 0]\n",
    "    defaults_train_both_years, defaults_test_both_years = train_test_split(defaults_both_years, test_size = 0.2, random_state = 1337)\n",
    "    completed_train_both_years, completed_test_both_years = train_test_split(completed_both_years,\n",
    "                                                                             test_size = defaults_train_both_years.shape[0] / completed_both_years.shape[0],\n",
    "                                                                             random_state = 1337)\n",
    "    both_years_balanced = pd.concat([completed_test_both_years, defaults_train_both_years])\n",
    "    \n",
    "    return year1_balanced, both_years_balanced, x_test_year1, y_test_year1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_2013, train_balanced_2013_2014, x_test_2013, y_test_2013 = transition_df_prep(df_historical_clean, 2013, 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_df_pipeline(train, x_test):\n",
    "    train_copy = train.copy()\n",
    "    x_test_copy = x_test.copy()\n",
    "    \n",
    "    # Imputations\n",
    "    missing_cols = list(train_copy.columns[train_copy.isnull().any()])\n",
    "    imputations_df = pd.DataFrame()\n",
    "    for i in missing_cols:\n",
    "        new_column = pd.Series(train_copy[i].median(), name = i)\n",
    "        imputations_df = pd.concat([imputations_df, new_column], axis = 1)\n",
    "    \n",
    "    for i in missing_cols:\n",
    "        train_copy.loc[train_copy[i].isna(), i] = imputations_df[i].item()\n",
    "        x_test_copy.loc[x_test_copy[i].isna(), i] = imputations_df[i].item()\n",
    "        \n",
    "    # Normalization\n",
    "    remove = ['loan_status', 'id', 'issue_d', 'year', 'grade', 'sub_grade', 'term']\n",
    "    numerical_columns = list(train_copy.select_dtypes(include = ['float64', 'int64']).columns)\n",
    "    numerical_columns = [x for x in numerical_columns if x not in remove]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_copy[numerical_columns] = scaler.fit_transform(train_copy[numerical_columns])\n",
    "    x_test_copy[numerical_columns] = scaler.transform(x_test_copy[numerical_columns])\n",
    "\n",
    "    # One hot encoding\n",
    "    \n",
    "    \n",
    "    return train_copy, x_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = transition_df_pipeline(train_balanced_2013, x_test_2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputations_df = pd.DataFrame()\n",
    "for i in list(df_2014.columns[df_2014.isnull().any()]):\n",
    "    new_column = pd.Series([df_2014[i].median()], name = i)\n",
    "    imputations_df = pd.concat([imputations_df, new_column], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputations_df['emp_length'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emp_length', 'revol_util'], dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2014.columns[df_2014.isnull().any()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_pipeline(df, test_set = False):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "issue_d                     0\n",
       "year                        0\n",
       "grade                       0\n",
       "sub_grade                   0\n",
       "loan_status                 0\n",
       "loan_amnt                   0\n",
       "term                        0\n",
       "int_rate                    0\n",
       "installment                 0\n",
       "emp_length              12019\n",
       "home_ownership              0\n",
       "annual_inc                  0\n",
       "verification_status         0\n",
       "purpose                     0\n",
       "addr_state                  0\n",
       "dti                         0\n",
       "earliest_cr_line            0\n",
       "fico_range_low              0\n",
       "open_acc                    0\n",
       "pub_rec                     0\n",
       "revol_bal                   0\n",
       "revol_util                125\n",
       "initial_list_status         0\n",
       "application_type            0\n",
       "mort_acc                    0\n",
       "pub_rec_bankruptcies        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2014.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "to_drop = ['loan_status', 'id', 'issue_d', 'year', 'grade', 'sub_grade']\n",
    "\n",
    "train_balanced = pd.read_csv('/Users/vinh/FS/thesis/data/train_balanced.csv')\n",
    "x_train_balanced = train_balanced.drop(to_drop, axis = 1)\n",
    "y_train_balanced = train_balanced[['loan_status']]\n",
    "\n",
    "val_final = pd.read_csv('/Users/vinh/FS/thesis/data/val_final.csv')\n",
    "x_val = val_final.drop(to_drop, axis = 1)\n",
    "y_val = val_final[['loan_status']]\n",
    "\n",
    "test_final = pd.read_csv('/Users/vinh/FS/thesis/data/test_final.csv')\n",
    "x_test = test_final.drop(to_drop, axis = 1)\n",
    "y_test = test_final[['loan_status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
